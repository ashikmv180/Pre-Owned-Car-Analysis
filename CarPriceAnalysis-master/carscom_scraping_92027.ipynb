{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images and data from cars.com for selected years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import cPickle\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "\n",
    "chromedriver = \"/Users/Chris/Desktop/DSI/chromedriver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base search URL is:\n",
    "https://www.cars.com/for-sale/searchresults.action?zc=92027&rd=75&prMn=5000&prMx=1000000&stkTypId=28881&yrId=58487&photoId=46724&sf1Nm=location&sf1Dir=ASC&sf2Nm=price&sf2Dir=DESC&page=1&perPage=100&sortFeatures=buryUsedLowPrice&sortFeatures=buryNewLowPrice&sortFeatures=buryLowPriceOlderThanSix&sortFeatures=buryNoPrice&searchSource=GN_REFINEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### may need to allow scripts to run on the browser that pops up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic outline of top-level scraping of cars.com for vehicle overview id numbers for later use\n",
    "\n",
    "- search within 75 miles of zipcode\n",
    "- Loops through each year between 2005:2012\n",
    "- price: min 5000, max:1000000\n",
    "- ad has photo\n",
    "- return 100 listings per page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to collect listings for a given zipcode with the parameters listed above\n",
    "# return a data frame and a directory full of images\n",
    "def get_listings_by_zipcode(zc,year_id):\n",
    "\n",
    "    # prepare the output df\n",
    "    detail_cols = ['ad_id', 'bodyStyle', 'city', 'dealerName', 'doorCount', 'downloadDate', 'drivetrain',\n",
    "               'engine', 'exteriorColor', 'fuelType', 'interiorColor', 'isGhostPhoto', 'isStockPhoto',\n",
    "               'listingDate', 'location', 'makeName', 'miles', 'modelName', 'modelYear', 'mpgCity',\n",
    "               'mpgHwy', 'oneOwner', 'photoUrlsLarge', 'price', 'privatePartyListing', 'seatCapacity',\n",
    "               'sellerNotesPt1', 'sellerNotesPt2', 'sellerPhotoCount', 'state', 'transmission', 'trimName',\n",
    "               'vin', 'zipcode']\n",
    "    \n",
    "    listings_df = pd.DataFrame(columns=detail_cols)\n",
    "\n",
    "    \n",
    "    # set the photo directory\n",
    "    img_base = '/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_images/'\n",
    "    img_dir = img_base + str(zc)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(img_dir)\n",
    "    except:\n",
    "        print img_dir,' already exists!'\n",
    "    \n",
    "    # count how many items are collected\n",
    "    listing_count = 0\n",
    "    image_count = 0\n",
    "    \n",
    "    # create empty lists to store the output\n",
    "    makeName, modelName, modelYear, trimName = [],[],[],[]\n",
    "    price, miles, exteriorColor, interiorColor = [],[],[],[]\n",
    "    bodyStyle, doorCount, transmission, drivetrain = [],[],[],[]\n",
    "    engine, fuelType,mpgCity,mpgHwy = [],[],[],[]\n",
    "    city,dealerName,privatePartyListing,state = [],[],[],[]\n",
    "    vin,zipcode,location,seatCapacity = [],[],[],[]\n",
    "    listingDate,photoUrlsLarge,sellerPhotoCount = [],[],[]\n",
    "    oneOwner,sellerNotesPt1,sellerNotesPt2 = [],[],[]\n",
    "    isStockPhoto,isGhostPhoto,downloadDate,ad_id = [],[],[],[]\n",
    "    \n",
    "    \n",
    "#     # get year ids so that the search can be broken down into smaller groups\n",
    "#     year_id_string = '58487&yrId=56007&yrId=51683&yrId=47272&yrId=39723&yrId=34923&yrId=27381&yrId=20201&yrId=20145&yrId=20200&yrId=20144&yrId=20199'\n",
    "#     years_ids = year_id_string.split('&yrId=')\n",
    "\n",
    "#     open a new browser instance\n",
    "#     browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "#     for page in pages_to_follow:\n",
    "#     for year_id in years_ids:\n",
    "        \n",
    "    # create the page to follow\n",
    "    pt1 = \"https://www.cars.com/for-sale/searchresults.action?zc=\"\n",
    "    pt2 = str(zc)\n",
    "    pt3 = \"&rd=75&prMn=5000&stkTypId=28881&yrId=\"\n",
    "    pt4 = str(year_id)\n",
    "    pt5 = \"&photoId=46724&sf1Nm=price&sf1Dir=DESC&sf2Nm=miles&sf2Dir=ASC&page=\" \n",
    "    pt6 = '1'\n",
    "    # temp set to 10 per page for training, 100 for collecting\n",
    "    pt7 = \"&perPage=100&sortFeatures=buryUsedLowPrice&sortFeatures=buryNewLowPrice&sortFeatures=buryLowPriceOlderThanSix&sortFeatures=buryNoPrice&sortFeatures=buryUsedLowMileage&searchSource=GN_REFINEMENT\"\n",
    "    link = pt1 + pt2 +  pt3 +  pt4 + pt5 + pt6 + pt7\n",
    "\n",
    "    # follow the link and get the html\n",
    "    browser.get(link)\n",
    "    time.sleep(9)\n",
    "    HTML = browser.page_source\n",
    "    soup = BeautifulSoup(HTML)\n",
    "\n",
    "    # get the number of listings for the year from heading1\n",
    "    # use this to determine how many pages to follow\n",
    "    num_items = soup.h1.get_text().split(' ')[0].replace(',','')\n",
    "    # add one for np.arange so that we'll get the last page\n",
    "    num_pages = int(num_items)/100 + 1\n",
    "\n",
    "    # make sure that there are no more than 50 pages, as that's the limit\n",
    "    if num_pages > 50:\n",
    "        num_pages = 50\n",
    "\n",
    "    # make the list of pages for creating the links in the loop below\n",
    "    pages = np.arange(1,num_pages,1)\n",
    "\n",
    "    print 'there are %s listings over %s pages for year_id %s' % (num_items,num_pages,year_id)\n",
    "    \n",
    "    # limit pages for testing...\n",
    "    #pages = [1]\n",
    "\n",
    "    # iterate through each page and collect the ids\n",
    "    for page in pages:\n",
    "        pt6 = str(page)\n",
    "        link = pt1 + pt2 +  pt3 +  pt4 + pt5 + pt6 + pt7\n",
    "\n",
    "        print 'now parsing page %s at %s' % (page, str(time.ctime()))\n",
    "        \n",
    "        # empty list to store the overview_ids\n",
    "        overview_list = []\n",
    "\n",
    "        browser.get(link)\n",
    "#         timeout = 10\n",
    "#         try:\n",
    "#             element_present = EC.presence_of_element_located((By.ID, 'h1'))\n",
    "#             WebDriverWait(browser, timeout).until(element_present)\n",
    "#         except TimeoutException:\n",
    "#             print \"Timed out waiting for page to load\"\n",
    "\n",
    "        time.sleep(9)\n",
    "        HTML = browser.page_source\n",
    "        soup = BeautifulSoup(HTML)\n",
    "\n",
    "        # go through each link in the html and pull out the overview id\n",
    "        # there are multiple occurences of each overview id, so will use\n",
    "        # np.unique later to get the unique list\n",
    "        for row in soup.find_all('a',href=True):\n",
    "            try: \n",
    "                overview_id = row['href'].split('/')[3]\n",
    "                if len(overview_id) == 9:\n",
    "                    overview_list.append(overview_id)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # only keep unique values in list\n",
    "        overview_list = np.unique(overview_list)\n",
    "\n",
    "        # extract the information for listings on each page\n",
    "        for i,id_num in enumerate(overview_list):\n",
    "\n",
    "            # prepare the links\n",
    "            ajax_start = 'https://www.cars.com/ajax/listingsapi/1.0/listing/detail/'\n",
    "            ajax_full = ajax_start + str(id_num)\n",
    "\n",
    "            # use requests to get the listing\n",
    "            # need to use try/except as some of them fail to load\n",
    "            try:\n",
    "                r = requests.get(ajax_full)\n",
    "                # add a random time delay to keep from getting kicked off...\n",
    "                sleep_time = np.random.randint(3,7)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            except:\n",
    "                print 'unable to access %s at listing count %s' % (ajax_full,listing_count)\n",
    "                time.sleep(10)\n",
    "                pass\n",
    "                \n",
    "            # listing is in json format, and all details are in first dict\n",
    "            try:\n",
    "                overview = r.json()['listingDetailDto']\n",
    "                listing_count += 1\n",
    "                #print overview\n",
    "            except:\n",
    "                #print 'this one failed:',ajax_full\n",
    "                #json_fail_count += 1\n",
    "                pass\n",
    "            \n",
    "            if (listing_count % 250) == 0:\n",
    "                print 'parsed %s listings' % listing_count\n",
    "\n",
    "\n",
    "            # fill the lists\n",
    "            # empty values are null\n",
    "            makeName.append(overview['makeName'])\n",
    "            modelName.append(overview['modelName'])\n",
    "            modelYear.append(overview['modelYear'])\n",
    "            trimName.append(overview['trimName'])\n",
    "            price.append(overview['price'])\n",
    "            miles.append(overview['milesInteger'])\n",
    "            exteriorColor.append(overview['exteriorColor'])\n",
    "            interiorColor.append(overview['interiorColor'])\n",
    "            bodyStyle.append(overview['bodystyleName'])\n",
    "            doorCount.append(overview['doorCount'])\n",
    "            transmission.append(overview['transmission'])\n",
    "            drivetrain.append(overview['drivetrain'])\n",
    "            engine.append(overview['engineDescription'])\n",
    "            fuelType.append(overview['fuelType'])\n",
    "            mpgCity.append(overview['mpgCity'])\n",
    "            mpgHwy.append(overview['mpgHwy'])\n",
    "            city.append(overview['city'])\n",
    "            dealerName.append(overview['dealerName'])\n",
    "            privatePartyListing.append(overview['privatePartyListing'])\n",
    "            state.append(overview['state'])\n",
    "            vin.append(overview['vin'])\n",
    "            zipcode.append(overview['zipcode'])\n",
    "            location.append(overview['location'])\n",
    "            seatCapacity.append(overview['seatCapacity'])\n",
    "            listingDate.append(time.ctime(overview['listingDateSeconds']))\n",
    "            photoUrlsLarge.append(overview['photoUrlsLarge'])\n",
    "            sellerPhotoCount.append(overview['sellerPhotoCount'])\n",
    "            oneOwner.append(overview['oneOwner'])\n",
    "            sellerNotesPt1.append(overview['sellerNotesPart1'])\n",
    "            sellerNotesPt2.append(overview['sellerNotesPart2'])\n",
    "            isStockPhoto.append(overview['isStockPhoto'])\n",
    "            isGhostPhoto.append(overview['isGhostPhoto'])\n",
    "            ad_id.append(id_num)\n",
    "            downloadDate.append(time.strftime(\"%m-%d-%Y\", time.localtime()))\n",
    "\n",
    "\n",
    "            # first check that the images will exist and be reasonable\n",
    "            if (((len(overview['photoUrlsLarge'])) > 2) & (overview['isGhostPhoto'] == False) & (overview['isStockPhoto'] == False) & (overview['makeName'] != None) & (overview['modelName'] != None)):\n",
    "\n",
    "                # now iterate through the list\n",
    "                # limit to first 5 photos to minimize number of interior images\n",
    "                # and to speed things up and save space\n",
    "                for num,photo in enumerate(overview['photoUrlsLarge'][0:5]):\n",
    "\n",
    "                    # create file name\n",
    "                    photo_name = overview['makeName'].lower() + '_' + overview['modelName'].lower() + '_' + str(overview['modelYear'])+ '_' + str(id_num) + '_' + str(num) + '.jpg'\n",
    "\n",
    "                    # full file name for photo with directory\n",
    "                    photo_file = img_dir + '/' + photo_name\n",
    "\n",
    "                    # see if it already exists\n",
    "                    \n",
    "                    if os.path.exists(photo_file):\n",
    "                        pass\n",
    "                    else:\n",
    "\n",
    "                        try:\n",
    "                            f = open(photo_file,'wb')\n",
    "                            f.write(requests.get(photo).content)\n",
    "                            f.close()\n",
    "                            image_count += 1\n",
    "                            if (image_count % 500) == 0:\n",
    "                                print 'at image %s' % image_count\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "        # save the results from each page into a temp data frame\n",
    "\n",
    "        # create a dataframe\n",
    "        temp_df = pd.DataFrame({\n",
    "        'makeName':makeName,\n",
    "        'modelName':modelName,\n",
    "        'modelYear':modelYear,\n",
    "        'trimName':trimName,\n",
    "        'price':price,\n",
    "        'miles':miles,\n",
    "        'exteriorColor':exteriorColor,\n",
    "        'interiorColor':interiorColor,\n",
    "        'bodyStyle':bodyStyle,\n",
    "        'doorCount':doorCount,\n",
    "        'transmission':transmission,\n",
    "        'drivetrain':drivetrain,\n",
    "        'engine':engine,\n",
    "        'fuelType':fuelType,\n",
    "        'mpgCity':mpgCity,\n",
    "        'mpgHwy':mpgHwy,\n",
    "        'city':city,\n",
    "        'dealerName':dealerName,\n",
    "        'privatePartyListing':privatePartyListing,\n",
    "        'state':state,\n",
    "        'vin':vin,\n",
    "        'zipcode':zipcode,\n",
    "        'location':location,\n",
    "        'seatCapacity':seatCapacity,\n",
    "        'listingDate':listingDate,\n",
    "        'photoUrlsLarge':photoUrlsLarge,\n",
    "        'sellerPhotoCount':sellerPhotoCount,\n",
    "        'oneOwner':oneOwner,\n",
    "        'sellerNotesPt1':sellerNotesPt1,\n",
    "        'sellerNotesPt2':sellerNotesPt2,\n",
    "        'isStockPhoto':isStockPhoto,\n",
    "        'isGhostPhoto':isGhostPhoto,\n",
    "        'downloadDate':downloadDate,\n",
    "        'ad_id':ad_id\n",
    "        #'photoBase':photo_base\n",
    "        })\n",
    "\n",
    "        # save it to file after every page just in case...\n",
    "        f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_listings/temp_listing_df.p','w')  # w = write, r = read\n",
    "        cPickle.dump(temp_df,f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    print \"parsed year_id %s, %s records and %s photos collected\" % (year_id,listing_count,image_count)\n",
    "\n",
    "    # concatentate all the listings\n",
    "    listings_df = pd.concat([listings_df,temp_df],axis=0)\n",
    "    \n",
    "    # close the browser\n",
    "    #browser.quit()\n",
    "\n",
    "\n",
    "\n",
    "    return listings_df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['58487', '56007', '51683']\n",
      "['39723', '34923', '27381']\n",
      "['20201', '20145', '20200']\n",
      "['20144', '20199']\n"
     ]
    }
   ],
   "source": [
    "# get year ids so that the search can be broken down into smaller groups\n",
    "year_id_string = '58487&yrId=56007&yrId=51683&yrId=47272&yrId=39723&yrId=34923&yrId=27381&yrId=20201&yrId=20145&yrId=20200&yrId=20144&yrId=20199'\n",
    "years_ids = year_id_string.split('&yrId=')\n",
    "\n",
    "print years_ids[0:3]\n",
    "print years_ids[4:7]\n",
    "print years_ids[7:10]\n",
    "print years_ids[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 2843 listings over 29 pages for year_id 58487\n",
      "now parsing page 1 at Sun Sep 18 07:22:05 2016\n",
      "now parsing page 2 at Sun Sep 18 07:35:30 2016\n",
      "now parsing page 3 at Sun Sep 18 07:35:58 2016\n",
      "at image 500\n",
      "now parsing page 4 at Sun Sep 18 07:50:13 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 5 at Sun Sep 18 08:04:44 2016\n",
      "at image 1500\n",
      "now parsing page 6 at Sun Sep 18 08:18:54 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 7 at Sun Sep 18 08:32:17 2016\n",
      "at image 2500\n",
      "now parsing page 8 at Sun Sep 18 08:46:47 2016\n",
      "at image 3000\n",
      "now parsing page 9 at Sun Sep 18 09:01:07 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 10 at Sun Sep 18 09:15:19 2016\n",
      "at image 4000\n",
      "now parsing page 11 at Sun Sep 18 09:29:01 2016\n",
      "parsed 1000 listings\n",
      "now parsing page 12 at Sun Sep 18 09:43:25 2016\n",
      "at image 4500\n",
      "now parsing page 13 at Sun Sep 18 09:56:52 2016\n",
      "at image 5000\n",
      "now parsing page 14 at Sun Sep 18 10:10:49 2016\n",
      "at image 5500\n",
      "now parsing page 15 at Sun Sep 18 10:21:52 2016\n",
      "parsed 1250 listings\n",
      "at image 6000\n",
      "now parsing page 16 at Sun Sep 18 10:36:07 2016\n",
      "at image 6500\n",
      "now parsing page 17 at Sun Sep 18 10:49:38 2016\n",
      "parsed 1500 listings\n",
      "at image 7000\n",
      "now parsing page 18 at Sun Sep 18 11:03:22 2016\n",
      "at image 7500\n",
      "now parsing page 19 at Sun Sep 18 11:16:50 2016\n",
      "now parsing page 20 at Sun Sep 18 11:30:18 2016\n",
      "parsed 1750 listings\n",
      "at image 8000\n",
      "now parsing page 21 at Sun Sep 18 11:44:34 2016\n",
      "at image 8500\n",
      "now parsing page 22 at Sun Sep 18 11:58:21 2016\n",
      "parsed 2000 listings\n",
      "at image 9000\n",
      "now parsing page 23 at Sun Sep 18 12:13:19 2016\n",
      "at image 9500\n",
      "now parsing page 24 at Sun Sep 18 12:27:23 2016\n",
      "parsed 2250 listings\n",
      "now parsing page 25 at Sun Sep 18 12:41:36 2016\n",
      "at image 10000\n",
      "now parsing page 26 at Sun Sep 18 12:55:43 2016\n",
      "at image 10500\n",
      "now parsing page 27 at Sun Sep 18 13:10:13 2016\n",
      "parsed 2500 listings\n",
      "at image 11000\n",
      "now parsing page 28 at Sun Sep 18 13:24:44 2016\n",
      "at image 11500\n",
      "parsed year_id 58487, 2670 records and 11650 photos collected\n",
      "(2781, 34)\n"
     ]
    }
   ],
   "source": [
    "# # open a new browser instance\n",
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "\n",
    "# listings_table_92027_58487 = get_listings_by_zipcode(92027,58487)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_58487.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_58487,f)\n",
    "# f.close()\n",
    "\n",
    "# print listings_table_92027_58487.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 5321 listings over 50 pages for year_id 56007\n",
      "now parsing page 1 at Sun Sep 18 13:47:00 2016\n",
      "now parsing page 2 at Sun Sep 18 14:01:28 2016\n",
      "at image 500\n",
      "now parsing page 3 at Sun Sep 18 14:16:13 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Sun Sep 18 14:31:12 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Sun Sep 18 14:46:16 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 6 at Sun Sep 18 14:59:37 2016\n",
      "at image 2500\n",
      "now parsing page 7 at Sun Sep 18 15:14:01 2016\n",
      "at image 3000\n",
      "now parsing page 8 at Sun Sep 18 15:27:51 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 9 at Sun Sep 18 15:42:09 2016\n",
      "at image 4000\n",
      "now parsing page 10 at Sun Sep 18 15:56:28 2016\n",
      "parsed 1000 listings\n",
      "at image 4500\n",
      "now parsing page 11 at Sun Sep 18 16:11:03 2016\n",
      "now parsing page 12 at Sun Sep 18 16:25:40 2016\n",
      "at image 5000\n",
      "now parsing page 13 at Sun Sep 18 16:40:59 2016\n",
      "parsed 1250 listings\n",
      "at image 5500\n",
      "now parsing page 14 at Sun Sep 18 16:53:49 2016\n",
      "at image 6000\n",
      "now parsing page 15 at Sun Sep 18 17:07:50 2016\n",
      "at image 6500\n",
      "parsed 1500 listings\n",
      "now parsing page 16 at Sun Sep 18 17:21:24 2016\n",
      "at image 7000\n",
      "now parsing page 17 at Sun Sep 18 17:36:14 2016\n",
      "at image 7500\n",
      "now parsing page 18 at Sun Sep 18 17:50:31 2016\n",
      "parsed 1750 listings\n",
      "at image 8000\n",
      "now parsing page 19 at Sun Sep 18 18:04:40 2016\n",
      "at image 8500\n",
      "now parsing page 20 at Sun Sep 18 18:19:16 2016\n",
      "parsed 2000 listings\n",
      "now parsing page 21 at Sun Sep 18 18:33:15 2016\n",
      "at image 9000\n",
      "now parsing page 22 at Sun Sep 18 18:47:14 2016\n",
      "at image 9500\n",
      "now parsing page 23 at Sun Sep 18 19:01:26 2016\n",
      "parsed 2250 listings\n",
      "at image 10000\n",
      "now parsing page 24 at Sun Sep 18 19:15:53 2016\n",
      "at image 10500\n",
      "now parsing page 25 at Sun Sep 18 19:30:13 2016\n",
      "at image 11000\n",
      "parsed 2500 listings\n",
      "now parsing page 26 at Sun Sep 18 19:45:28 2016\n",
      "at image 11500\n",
      "now parsing page 27 at Sun Sep 18 20:02:27 2016\n",
      "at image 12000\n",
      "now parsing page 28 at Sun Sep 18 20:18:30 2016\n",
      "parsed 2750 listings\n",
      "now parsing page 29 at Sun Sep 18 20:33:47 2016\n",
      "at image 12500\n",
      "now parsing page 30 at Sun Sep 18 20:49:49 2016\n",
      "at image 13000\n",
      "parsed 3000 listings\n",
      "now parsing page 31 at Sun Sep 18 21:05:51 2016\n",
      "at image 13500\n",
      "now parsing page 32 at Sun Sep 18 21:20:12 2016\n",
      "at image 14000\n",
      "parsed 3250 listings\n",
      "now parsing page 33 at Sun Sep 18 21:34:14 2016\n",
      "at image 14500\n",
      "now parsing page 34 at Sun Sep 18 21:48:38 2016\n",
      "at image 15000\n",
      "now parsing page 35 at Sun Sep 18 22:01:57 2016\n",
      "parsed 3500 listings\n",
      "at image 15500\n",
      "now parsing page 36 at Sun Sep 18 22:16:42 2016\n",
      "now parsing page 37 at Sun Sep 18 22:30:58 2016\n",
      "at image 16000\n",
      "parsed 3750 listings\n",
      "now parsing page 38 at Sun Sep 18 22:44:38 2016\n",
      "at image 16500\n",
      "now parsing page 39 at Sun Sep 18 22:59:22 2016\n",
      "at image 17000\n",
      "now parsing page 40 at Sun Sep 18 23:13:39 2016\n",
      "parsed 4000 listings\n",
      "at image 17500\n",
      "now parsing page 41 at Sun Sep 18 23:27:46 2016\n",
      "at image 18000\n",
      "now parsing page 42 at Sun Sep 18 23:41:27 2016\n",
      "parsed 4250 listings\n",
      "at image 18500\n",
      "now parsing page 43 at Sun Sep 18 23:54:55 2016\n",
      "at image 19000\n",
      "now parsing page 44 at Mon Sep 19 00:08:53 2016\n",
      "now parsing page 45 at Mon Sep 19 00:22:56 2016\n",
      "parsed 4500 listings\n",
      "at image 19500\n",
      "now parsing page 46 at Mon Sep 19 00:36:50 2016\n",
      "at image 20000\n",
      "now parsing page 47 at Mon Sep 19 00:50:50 2016\n",
      "at image 20500\n",
      "parsed 4750 listings\n",
      "now parsing page 48 at Mon Sep 19 01:06:00 2016\n",
      "at image 21000\n",
      "now parsing page 49 at Mon Sep 19 01:20:01 2016\n",
      "at image 21500\n",
      "parsed 5000 listings\n",
      "parsed year_id 56007, 5003 records and 21772 photos collected\n",
      "(5046, 34)\n"
     ]
    }
   ],
   "source": [
    "# # open a new browser instance\n",
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # done!\n",
    "# listings_table_92027_56007 = get_listings_by_zipcode(92027,56007)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_56007.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_56007,f)\n",
    "# f.close()\n",
    "\n",
    "# print listings_table_92027_56007.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 5018 listings over 50 pages for year_id 51683\n",
      "now parsing page 38 at Tue Sep 20 21:22:48 2016\n",
      "now parsing page 39 at Tue Sep 20 21:38:07 2016\n",
      "at image 500\n",
      "now parsing page 40 at Tue Sep 20 21:53:48 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 41 at Tue Sep 20 22:08:53 2016\n",
      "at image 1500\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/677487841 at listing count 361\n",
      "now parsing page 42 at Tue Sep 20 22:27:48 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 43 at Tue Sep 20 22:42:05 2016\n",
      "at image 2500\n",
      "now parsing page 44 at Tue Sep 20 22:55:18 2016\n",
      "now parsing page 45 at Tue Sep 20 23:08:46 2016\n",
      "at image 3000\n",
      "parsed 750 listings\n",
      "now parsing page 46 at Tue Sep 20 23:21:41 2016\n",
      "at image 3500\n",
      "now parsing page 47 at Tue Sep 20 23:34:56 2016\n",
      "at image 4000\n",
      "parsed 1000 listings\n",
      "now parsing page 48 at Tue Sep 20 23:48:02 2016\n",
      "at image 4500\n",
      "now parsing page 49 at Wed Sep 21 00:01:32 2016\n",
      "at image 5000\n",
      "parsed year_id 51683, 1236 records and 5277 photos collected\n",
      "(1236, 34)\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # had to split this one into two parts\n",
    "# # first listings are saved in listings_table_92027_51683a.p,listings_table_92027_51683b.p\n",
    "\n",
    "# # done!\n",
    "\n",
    "# # reset the page to 38 before starting this one\n",
    "# listings_table_92027_51683 = get_listings_by_zipcode(92027,51683)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_51683.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_51683,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# print listings_table_92027_51683.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 2543 listings over 26 pages for year_id 39723\n",
      "now parsing page 1 at Tue Sep 20 11:00:02 2016\n",
      "now parsing page 2 at Tue Sep 20 11:11:54 2016\n",
      "at image 500\n",
      "now parsing page 3 at Tue Sep 20 11:23:55 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Tue Sep 20 11:36:16 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Tue Sep 20 11:48:59 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 6 at Tue Sep 20 12:01:31 2016\n",
      "at image 2500\n",
      "now parsing page 7 at Tue Sep 20 12:15:50 2016\n",
      "at image 3000\n",
      "now parsing page 8 at Tue Sep 20 12:27:31 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 9 at Tue Sep 20 12:39:39 2016\n",
      "at image 4000\n",
      "now parsing page 10 at Tue Sep 20 12:52:09 2016\n",
      "at image 4500\n",
      "parsed 1000 listings\n",
      "now parsing page 11 at Tue Sep 20 13:04:47 2016\n",
      "at image 5000\n",
      "now parsing page 12 at Tue Sep 20 13:16:50 2016\n",
      "now parsing page 13 at Tue Sep 20 13:18:18 2016\n",
      "at image 5500\n",
      "now parsing page 14 at Tue Sep 20 13:31:20 2016\n",
      "parsed 1250 listings\n",
      "at image 6000\n",
      "now parsing page 15 at Tue Sep 20 13:44:14 2016\n",
      "now parsing page 16 at Tue Sep 20 13:56:22 2016\n",
      "now parsing page 17 at Tue Sep 20 13:56:51 2016\n",
      "now parsing page 18 at Tue Sep 20 13:57:17 2016\n",
      "now parsing page 19 at Tue Sep 20 13:57:43 2016\n",
      "now parsing page 20 at Tue Sep 20 13:58:28 2016\n",
      "now parsing page 21 at Tue Sep 20 13:58:54 2016\n",
      "now parsing page 22 at Tue Sep 20 13:59:20 2016\n",
      "now parsing page 23 at Tue Sep 20 13:59:46 2016\n",
      "now parsing page 24 at Tue Sep 20 14:00:12 2016\n",
      "now parsing page 25 at Tue Sep 20 14:00:37 2016\n",
      "parsed year_id 39723, 1425 records and 6470 photos collected\n",
      "(1442, 34)\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# listings_table_92027_39723 = get_listings_by_zipcode(92027,39723)\n",
    "\n",
    "# done for pages 1-14\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_39723.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_39723,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# print listings_table_92027_39723.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 2551 listings over 26 pages for year_id 39723\n",
      "now parsing page 15 at Tue Sep 20 14:03:51 2016\n",
      "now parsing page 16 at Tue Sep 20 14:12:32 2016\n",
      "now parsing page 17 at Tue Sep 20 14:25:27 2016\n",
      "at image 500\n",
      "parsed 250 listings\n",
      "now parsing page 18 at Tue Sep 20 14:38:02 2016\n",
      "at image 1000\n",
      "now parsing page 19 at Tue Sep 20 14:51:24 2016\n",
      "at image 1500\n",
      "parsed 500 listings\n",
      "now parsing page 20 at Tue Sep 20 15:03:28 2016\n",
      "at image 2000\n",
      "now parsing page 21 at Tue Sep 20 15:16:18 2016\n",
      "at image 2500\n",
      "now parsing page 22 at Tue Sep 20 15:29:21 2016\n",
      "parsed 750 listings\n",
      "at image 3000\n",
      "now parsing page 23 at Tue Sep 20 15:46:16 2016\n",
      "at image 3500\n",
      "now parsing page 24 at Tue Sep 20 15:58:27 2016\n",
      "at image 4000\n",
      "parsed 1000 listings\n",
      "now parsing page 25 at Tue Sep 20 16:10:43 2016\n",
      "at image 4500\n",
      "parsed year_id 39723, 1126 records and 4678 photos collected\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # repeat for pages 15-end\n",
    "# listings_table_92027_39723 = get_listings_by_zipcode(92027,39723)\n",
    "\n",
    "# # done!\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_39723.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_39723,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# # print listings_table_92027_39723.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 1765 listings over 18 pages for year_id 34923\n",
      "now parsing page 1 at Tue Sep 20 16:40:28 2016\n",
      "now parsing page 2 at Tue Sep 20 16:48:48 2016\n",
      "now parsing page 3 at Tue Sep 20 16:58:22 2016\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/672678850 at listing count 229\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/673045795 at listing count 233\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Tue Sep 20 18:06:03 2016\n",
      "now parsing page 5 at Tue Sep 20 18:06:24 2016\n",
      "now parsing page 6 at Tue Sep 20 18:19:55 2016\n",
      "at image 500\n",
      "parsed 500 listings\n",
      "now parsing page 7 at Tue Sep 20 18:34:33 2016\n",
      "at image 1000\n",
      "now parsing page 8 at Tue Sep 20 18:50:11 2016\n",
      "at image 1500\n",
      "now parsing page 9 at Tue Sep 20 19:05:41 2016\n",
      "parsed 750 listings\n",
      "at image 2000\n",
      "now parsing page 10 at Tue Sep 20 19:21:24 2016\n",
      "at image 2500\n",
      "now parsing page 11 at Tue Sep 20 19:37:03 2016\n",
      "at image 3000\n",
      "parsed 1000 listings\n",
      "now parsing page 12 at Tue Sep 20 19:52:43 2016\n",
      "at image 3500\n",
      "now parsing page 13 at Tue Sep 20 20:08:47 2016\n",
      "now parsing page 14 at Tue Sep 20 20:09:32 2016\n",
      "at image 4000\n",
      "now parsing page 15 at Tue Sep 20 20:26:50 2016\n",
      "parsed 1250 listings\n",
      "at image 4500\n",
      "now parsing page 16 at Tue Sep 20 20:42:45 2016\n",
      "at image 5000\n",
      "now parsing page 17 at Tue Sep 20 20:58:56 2016\n",
      "parsed 1500 listings\n",
      "at image 5500\n",
      "parsed year_id 34923, 1539 records and 5583 photos collected\n",
      "(1544, 34)\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "# listings_table_92027_34923 = get_listings_by_zipcode(92027,34923)\n",
    "\n",
    "# # done!\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_34923.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_34923,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "# print listings_table_92027_34923.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# #done!\n",
    "# listings_table_92027_27381 = get_listings_by_zipcode(92027,27381)\n",
    "\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_27381.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_27381,f)\n",
    "# f.close()\n",
    "\n",
    "# print listings_table_92027_27381.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # done!\n",
    "# listings_table_92027_20201 = get_listings_by_zipcode(92027,20201)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_20201.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_20201,f)\n",
    "# f.close()\n",
    "\n",
    "# print listings_table_92027_20201.shape\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 1350 listings over 14 pages for year_id 20145\n",
      "now parsing page 1 at Mon Sep 19 14:17:29 2016\n",
      "now parsing page 2 at Mon Sep 19 14:29:22 2016\n",
      "at image 500\n",
      "now parsing page 3 at Mon Sep 19 14:41:07 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Mon Sep 19 14:53:18 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Mon Sep 19 15:04:49 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 6 at Mon Sep 19 15:17:07 2016\n",
      "now parsing page 7 at Mon Sep 19 15:17:29 2016\n",
      "now parsing page 8 at Mon Sep 19 15:17:46 2016\n",
      "now parsing page 9 at Mon Sep 19 15:18:02 2016\n",
      "at image 2500\n",
      "now parsing page 10 at Mon Sep 19 15:30:08 2016\n",
      "at image 3000\n",
      "now parsing page 11 at Mon Sep 19 15:42:22 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 12 at Mon Sep 19 15:55:00 2016\n",
      "now parsing page 13 at Mon Sep 19 15:55:20 2016\n",
      "parsed year_id 20145, 822 records and 3718 photos collected\n",
      "(823, 34)\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# #done\n",
    "\n",
    "# listings_table_92027_20145 = get_listings_by_zipcode(92027,20145)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_20145.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_20145,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# print listings_table_92027_20145.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 1262 listings over 13 pages for year_id 20200\n",
      "now parsing page 1 at Mon Sep 19 16:07:49 2016\n",
      "now parsing page 2 at Mon Sep 19 16:20:09 2016\n",
      "at image 500\n",
      "now parsing page 3 at Mon Sep 19 16:32:20 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Mon Sep 19 16:45:07 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Mon Sep 19 16:57:48 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 6 at Mon Sep 19 17:23:58 2016\n",
      "at image 2500\n",
      "now parsing page 7 at Mon Sep 19 17:36:09 2016\n",
      "at image 3000\n",
      "now parsing page 8 at Mon Sep 19 17:50:25 2016\n",
      "now parsing page 9 at Mon Sep 19 17:50:44 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 10 at Mon Sep 19 18:03:31 2016\n",
      "at image 4000\n",
      "now parsing page 11 at Mon Sep 19 18:16:24 2016\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/674607224 at listing count 962\n",
      "at image 4500\n",
      "parsed 1000 listings\n",
      "now parsing page 12 at Mon Sep 19 19:20:48 2016\n",
      "at image 5000\n",
      "parsed year_id 20200, 1132 records and 5227 photos collected\n",
      "(1132, 34)\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# listings_table_92027_20200 = get_listings_by_zipcode(92027,20200)\n",
    "\n",
    "# # done!\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_20200.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_20200,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n",
    "\n",
    "# print listings_table_92027_20200.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 1023 listings over 11 pages for year_id 20144\n",
      "now parsing page 1 at Mon Sep 19 12:16:12 2016\n",
      "now parsing page 2 at Mon Sep 19 12:28:03 2016\n",
      "at image 500\n",
      "now parsing page 3 at Mon Sep 19 12:40:40 2016\n",
      "now parsing page 4 at Mon Sep 19 12:41:03 2016\n",
      "now parsing page 5 at Mon Sep 19 12:41:20 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 6 at Mon Sep 19 12:57:28 2016\n",
      "at image 1500\n",
      "now parsing page 7 at Mon Sep 19 13:15:15 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 8 at Mon Sep 19 13:28:01 2016\n",
      "at image 2500\n",
      "now parsing page 9 at Mon Sep 19 13:45:11 2016\n",
      "at image 3000\n",
      "now parsing page 10 at Mon Sep 19 13:57:51 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "parsed year_id 20144, 821 records and 3788 photos collected\n",
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_images/92027  already exists!\n",
      "there are 734 listings over 8 pages for year_id 20199\n",
      "now parsing page 1 at Mon Sep 19 14:11:32 2016\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/674767340 at listing count 29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9c73ebd74d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlistings_table_92027_20199\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_listings_by_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m92027\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20199\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4618108dc8ad>\u001b[0m in \u001b[0;36mget_listings_by_zipcode\u001b[0;34m(zc, year_id)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m'unable to access %s at listing count %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0majax_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlisting_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # done!\n",
    "# listings_table_92027_20144 = get_listings_by_zipcode(92027,20144)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_20144.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_20144,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/dsi/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Applications/anaconda/envs/dsi/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 731 listings over 8 pages for year_id 20199\n",
      "now parsing page 1 at Fri Sep 16 13:47:59 2016\n",
      "at image 500\n",
      "now parsing page 2 at Fri Sep 16 14:03:29 2016\n",
      "at image 1000\n",
      "at image 1500\n",
      "now parsing page 3 at Fri Sep 16 14:19:33 2016\n",
      "at image 2000\n",
      "parsed 250 listings\n",
      "at image 2500\n",
      "now parsing page 4 at Fri Sep 16 14:35:35 2016\n",
      "at image 3000\n",
      "at image 3500\n",
      "now parsing page 5 at Fri Sep 16 14:51:20 2016\n",
      "at image 4000\n",
      "parsed 500 listings\n",
      "at image 4500\n",
      "now parsing page 6 at Fri Sep 16 15:08:05 2016\n",
      "at image 5000\n",
      "now parsing page 7 at Fri Sep 16 15:24:47 2016\n",
      "parsed year_id 20199, 618 records and 5436 photos collected\n"
     ]
    }
   ],
   "source": [
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # done!\n",
    "# listings_table_92027_20199 = get_listings_by_zipcode(92027,20199)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom_listings/listings_table_92027_20199.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_20199,f)\n",
    "# f.close()\n",
    "\n",
    "# browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_images/92027  already exists!\n",
      "there are 5575 listings over 50 pages for year_id 47272\n",
      "now parsing page 1 at Wed Sep 21 20:51:42 2016\n",
      "now parsing page 2 at Wed Sep 21 21:05:40 2016\n",
      "at image 500\n",
      "now parsing page 3 at Wed Sep 21 21:18:29 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Wed Sep 21 21:31:49 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Wed Sep 21 21:44:39 2016\n",
      "at image 2000\n",
      "parsed 500 listings\n",
      "now parsing page 6 at Wed Sep 21 21:58:19 2016\n",
      "at image 2500\n",
      "now parsing page 7 at Wed Sep 21 22:12:31 2016\n",
      "at image 3000\n",
      "now parsing page 8 at Wed Sep 21 22:25:50 2016\n",
      "parsed 750 listings\n",
      "at image 3500\n",
      "now parsing page 9 at Wed Sep 21 22:38:06 2016\n",
      "at image 4000\n",
      "now parsing page 10 at Wed Sep 21 22:52:20 2016\n",
      "parsed 1000 listings\n",
      "at image 4500\n",
      "now parsing page 11 at Wed Sep 21 23:04:57 2016\n",
      "now parsing page 12 at Wed Sep 21 23:17:36 2016\n",
      "at image 5000\n",
      "now parsing page 13 at Wed Sep 21 23:30:16 2016\n",
      "parsed 1250 listings\n",
      "at image 5500\n",
      "now parsing page 14 at Wed Sep 21 23:43:16 2016\n",
      "at image 6000\n",
      "now parsing page 15 at Wed Sep 21 23:55:53 2016\n",
      "at image 6500\n",
      "parsed 1500 listings\n",
      "now parsing page 16 at Thu Sep 22 00:09:14 2016\n",
      "at image 7000\n",
      "now parsing page 17 at Thu Sep 22 00:21:36 2016\n",
      "at image 7500\n",
      "parsed 1750 listings\n",
      "now parsing page 18 at Thu Sep 22 00:34:22 2016\n",
      "at image 8000\n",
      "now parsing page 19 at Thu Sep 22 00:47:55 2016\n",
      "at image 8500\n",
      "now parsing page 20 at Thu Sep 22 01:01:15 2016\n",
      "parsed 2000 listings\n",
      "at image 9000\n",
      "now parsing page 21 at Thu Sep 22 01:15:19 2016\n",
      "now parsing page 22 at Thu Sep 22 01:29:30 2016\n",
      "at image 9500\n",
      "parsed 2250 listings\n",
      "now parsing page 23 at Thu Sep 22 01:45:08 2016\n",
      "at image 10000\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675132059 at listing count 2289\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675149494 at listing count 2290\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675411135 at listing count 2291\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675590421 at listing count 2292\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675660796 at listing count 2293\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675660799 at listing count 2294\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675679072 at listing count 2295\n",
      "now parsing page 24 at Thu Sep 22 02:43:06 2016\n",
      "at image 10500\n",
      "now parsing page 25 at Thu Sep 22 02:56:19 2016\n",
      "parsed 2500 listings\n",
      "at image 11000\n",
      "now parsing page 26 at Thu Sep 22 03:12:10 2016\n",
      "at image 11500\n",
      "now parsing page 27 at Thu Sep 22 03:26:18 2016\n",
      "parsed 2750 listings\n",
      "at image 12000\n",
      "now parsing page 28 at Thu Sep 22 03:38:48 2016\n",
      "now parsing page 29 at Thu Sep 22 03:51:43 2016\n",
      "at image 12500\n",
      "now parsing page 30 at Thu Sep 22 04:05:01 2016\n",
      "parsed 3000 listings\n",
      "at image 13000\n",
      "now parsing page 31 at Thu Sep 22 04:17:39 2016\n",
      "at image 13500\n",
      "now parsing page 32 at Thu Sep 22 04:30:17 2016\n",
      "parsed 3250 listings\n",
      "at image 14000\n",
      "now parsing page 33 at Thu Sep 22 04:42:55 2016\n",
      "at image 14500\n",
      "now parsing page 34 at Thu Sep 22 04:55:38 2016\n",
      "at image 15000\n",
      "now parsing page 35 at Thu Sep 22 05:08:27 2016\n",
      "parsed 3500 listings\n",
      "now parsing page 36 at Thu Sep 22 05:21:20 2016\n",
      "now parsing page 37 at Thu Sep 22 05:21:32 2016\n",
      "at image 15500\n",
      "now parsing page 38 at Thu Sep 22 05:35:13 2016\n",
      "at image 16000\n",
      "parsed 3750 listings\n",
      "now parsing page 39 at Thu Sep 22 05:48:10 2016\n",
      "at image 16500\n",
      "now parsing page 40 at Thu Sep 22 06:01:02 2016\n",
      "at image 17000\n",
      "now parsing page 41 at Thu Sep 22 06:13:43 2016\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "parsed 4000 listings\n",
      "at image 17500\n",
      "now parsing page 42 at Thu Sep 22 06:26:04 2016\n",
      "at image 18000\n",
      "now parsing page 43 at Thu Sep 22 06:38:20 2016\n",
      "at image 18500\n",
      "now parsing page 44 at Thu Sep 22 06:51:08 2016\n",
      "parsed 4250 listings\n",
      "at image 19000\n",
      "now parsing page 45 at Thu Sep 22 07:04:30 2016\n",
      "now parsing page 46 at Thu Sep 22 07:16:57 2016\n",
      "at image 19500\n",
      "parsed 4500 listings\n",
      "now parsing page 47 at Thu Sep 22 07:30:34 2016\n",
      "at image 20000\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/679798869 at listing count 4593\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/680096674 at listing count 4599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-14c8ca426def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# done!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlistings_table_92027_47272\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_listings_by_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m92027\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m47272\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# save the processed list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-34d73af5141a>\u001b[0m in \u001b[0;36mget_listings_by_zipcode\u001b[0;34m(zc, year_id)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m'unable to access %s at listing count %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0majax_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlisting_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # forgot to collect year 2013!!\n",
    "# # 47272\n",
    "# # 92027\n",
    "\n",
    "# browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# # done!\n",
    "# listings_table_92027_47272 = get_listings_by_zipcode(92027,47272)\n",
    "\n",
    "# # save the processed list\n",
    "# f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_listings/listings_table_92027_47272.p','w')  # w = write, r = read\n",
    "# cPickle.dump(listings_table_92027_47272,f)\n",
    "# f.close()\n",
    "\n",
    "# #browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_images/94066  already exists!\n",
      "there are 5409 listings over 50 pages for year_id 47272\n",
      "now parsing page 1 at Thu Sep 22 09:01:13 2016\n",
      "now parsing page 2 at Thu Sep 22 09:13:24 2016\n",
      "at image 500\n",
      "now parsing page 3 at Thu Sep 22 09:24:45 2016\n",
      "at image 1000\n",
      "parsed 250 listings\n",
      "now parsing page 4 at Thu Sep 22 09:36:31 2016\n",
      "at image 1500\n",
      "now parsing page 5 at Thu Sep 22 09:48:26 2016\n",
      "at image 2000\n",
      "now parsing page 6 at Thu Sep 22 09:59:31 2016\n",
      "at image 2500\n",
      "now parsing page 7 at Thu Sep 22 10:09:29 2016\n",
      "at image 3000\n",
      "now parsing page 8 at Thu Sep 22 10:18:46 2016\n",
      "at image 3500\n",
      "now parsing page 9 at Thu Sep 22 10:27:58 2016\n",
      "at image 4000\n",
      "now parsing page 10 at Thu Sep 22 10:36:45 2016\n",
      "at image 4500\n",
      "now parsing page 11 at Thu Sep 22 10:45:45 2016\n",
      "at image 5000\n",
      "now parsing page 12 at Thu Sep 22 10:54:53 2016\n",
      "parsed 500 listings\n",
      "at image 5500\n",
      "now parsing page 13 at Thu Sep 22 11:06:53 2016\n",
      "at image 6000\n",
      "now parsing page 14 at Thu Sep 22 11:18:58 2016\n",
      "parsed 750 listings\n",
      "now parsing page 15 at Thu Sep 22 11:31:03 2016\n",
      "at image 6500\n",
      "now parsing page 16 at Thu Sep 22 11:42:54 2016\n",
      "at image 7000\n",
      "now parsing page 17 at Thu Sep 22 11:54:46 2016\n",
      "parsed 1000 listings\n",
      "at image 7500\n",
      "now parsing page 18 at Thu Sep 22 12:12:09 2016\n",
      "at image 8000\n",
      "now parsing page 19 at Thu Sep 22 12:24:02 2016\n",
      "at image 8500\n",
      "parsed 1250 listings\n",
      "now parsing page 20 at Thu Sep 22 12:35:59 2016\n",
      "at image 9000\n",
      "now parsing page 21 at Thu Sep 22 12:48:10 2016\n",
      "at image 9500\n",
      "parsed 1500 listings\n",
      "now parsing page 22 at Thu Sep 22 12:59:57 2016\n",
      "at image 10000\n",
      "now parsing page 23 at Thu Sep 22 13:11:51 2016\n",
      "at image 10500\n",
      "now parsing page 24 at Thu Sep 22 13:23:35 2016\n",
      "parsed 1750 listings\n",
      "now parsing page 25 at Thu Sep 22 13:34:56 2016\n",
      "at image 11000\n",
      "now parsing page 26 at Thu Sep 22 13:47:18 2016\n",
      "at image 11500\n",
      "now parsing page 27 at Thu Sep 22 13:58:40 2016\n",
      "parsed 2000 listings\n",
      "at image 12000\n",
      "now parsing page 28 at Thu Sep 22 14:11:22 2016\n",
      "at image 12500\n",
      "now parsing page 29 at Thu Sep 22 14:23:20 2016\n",
      "parsed 2250 listings\n",
      "at image 13000\n",
      "now parsing page 30 at Thu Sep 22 14:35:20 2016\n",
      "at image 13500\n",
      "now parsing page 31 at Thu Sep 22 14:47:29 2016\n",
      "at image 14000\n",
      "now parsing page 32 at Thu Sep 22 14:59:29 2016\n",
      "parsed 2500 listings\n",
      "at image 14500\n",
      "now parsing page 33 at Thu Sep 22 15:11:55 2016\n",
      "at image 15000\n",
      "now parsing page 34 at Thu Sep 22 15:24:17 2016\n",
      "parsed 2750 listings\n",
      "at image 15500\n",
      "now parsing page 35 at Thu Sep 22 15:36:12 2016\n",
      "now parsing page 36 at Thu Sep 22 15:49:23 2016\n",
      "at image 16000\n",
      "parsed 3000 listings\n",
      "now parsing page 37 at Thu Sep 22 16:01:55 2016\n",
      "at image 16500\n",
      "now parsing page 38 at Thu Sep 22 16:14:03 2016\n",
      "at image 17000\n",
      "now parsing page 39 at Thu Sep 22 16:26:15 2016\n",
      "at image 17500\n",
      "parsed 3250 listings\n",
      "now parsing page 40 at Thu Sep 22 16:36:51 2016\n",
      "at image 18000\n",
      "now parsing page 41 at Thu Sep 22 16:48:24 2016\n",
      "at image 18500\n",
      "now parsing page 42 at Thu Sep 22 17:00:39 2016\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675805572 at listing count 3475\n",
      "unable to access https://www.cars.com/ajax/listingsapi/1.0/listing/detail/675812596 at listing count 3477\n",
      "parsed 3500 listings\n",
      "at image 19000\n",
      "now parsing page 43 at Thu Sep 22 18:16:04 2016\n",
      "at image 19500\n",
      "now parsing page 44 at Thu Sep 22 18:29:37 2016\n",
      "parsed 3750 listings\n",
      "at image 20000\n",
      "now parsing page 45 at Thu Sep 22 18:42:59 2016\n",
      "now parsing page 46 at Thu Sep 22 18:57:08 2016\n",
      "at image 20500\n",
      "now parsing page 47 at Thu Sep 22 19:12:05 2016\n",
      "at image 21000\n",
      "parsed 4000 listings\n",
      "now parsing page 48 at Thu Sep 22 19:26:13 2016\n",
      "at image 21500\n",
      "now parsing page 49 at Thu Sep 22 19:40:14 2016\n",
      "at image 22000\n",
      "parsed 4250 listings\n",
      "parsed year_id 47272, 4278 records and 22224 photos collected\n"
     ]
    }
   ],
   "source": [
    "# forgot to collect year 2013!!\n",
    "# 47272\n",
    "# 94066\n",
    "\n",
    "browser = webdriver.Chrome(executable_path = chromedriver)\n",
    "\n",
    "# done!\n",
    "listings_table_94066_47272 = get_listings_by_zipcode(94066,47272)\n",
    "\n",
    "# save the processed list\n",
    "f = open('/Users/Chris/Desktop/DSI-SF-2-clrife/capstone/data/carscom/carscom_listings/listings_table_94066_47272.p','w')  # w = write, r = read\n",
    "cPickle.dump(listings_table_94066_47272,f)\n",
    "f.close()\n",
    "\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
